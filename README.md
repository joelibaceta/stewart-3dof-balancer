# Stewart PPO Agent

Este proyecto implementa un sistema de control basado en Aprendizaje por Refuerzo (RL) para una plataforma de **3 grados de libertad (3DoF)** tipo *Stewart Platform* que debe mantener una bola lo m√°s cerca posible del centro de la superficie.

Est√° construido sobre PyTorch y Gymnasium, e incluye:
- entorno visual simulado,
- control continuo sobre servos,
- entrenamiento usando **Proximal Policy Optimization (PPO)**.


## Problema a resolver
- **Ambiente**: plataforma rob√≥tica tipo Stewart (simulada), 3DOF.
- **Observaci√≥n**: imagen RGB top-down del entorno.
- **Acciones**: comandos continuos a tres servos.
- **Recompensa**: proporcional a la distancia entre la esfera y el centro de la plataforma.

## Justificaci√≥n te√≥rica: ¬øPor qu√© usar Actor-Critic?

La elecci√≥n de un enfoque Actor-Critic se basa en los siguientes criterios, derivados de la teor√≠a de DRL estudiada en clase (ver presentaci√≥n DRL 5.1):

- Espacio de acci√≥n continuo
    - Algoritmos como DQN son inapropiados, pues discretizar el espacio de control para 3 motores ser√≠a poco eficiente y escalar√≠a mal.
    - Actor-Critic permite modelar pol√≠ticas estoc√°sticas sobre acciones continuas.

- Estabilidad y aprendizaje m√°s eficiente
    - Al combinar estimaci√≥n de valor (Critic) con pol√≠ticas directas (Actor), se reduce la varianza del gradiente de pol√≠tica.
    - Se evita el alto sesgo de los m√©todos puramente basados en valor (como TD learning).

- Aprendizaje end-to-end con percepci√≥n visual
    - Se utilizan CNNs para procesar directamente las im√°genes, lo cual hace viable un aprendizaje desde la percepci√≥n hasta el control motor.

## ‚öôÔ∏è Estructura del proyecto


## üéØ Objetivo del proyecto

Entrenar un agente RL que, observando im√°genes de c√°mara (visi√≥n), aprenda a mover la plataforma para **mantener la bola estable en el centro**.

- Observaciones: stack de 4 im√°genes (dim: `12x84x84`)
- Acciones: cambios angulares (`ŒîŒ∏`) para los 3 servos (`Box(-1, 1)^3` ‚Üí escalado a radianes)
- Recompensa: funci√≥n densa basada en:
  - cercan√≠a al centro,
  - penalizaci√≥n por cambios bruscos de acci√≥n,
  - estar dentro o fuera del √°rea objetivo.


## üìä Arquitectura General

### Componentes del Agente

Este agente implementa el algoritmo PPO para entornos con espacios de acci√≥n continuos. A continuaci√≥n, se presenta una tabla que resume sus principales componentes:

| Componente         | Descripci√≥n                                                                                       | Ubicaci√≥n en el c√≥digo               |
|--------------------|---------------------------------------------------------------------------------------------------|--------------------------------------|
| **ActorCriticCNN** | Red neuronal que integra actor y cr√≠tico. El actor predice una distribuci√≥n gaussiana para muestrear acciones; el cr√≠tico estima el valor del estado. | `models/actor_critic_cnn.py`         |
| **PPOAgent**       | Orquesta la interacci√≥n con el entorno, recolecta experiencias, administra buffers y coordina el entrenamiento. | `agents/ppo_agent.py`                |
| **PPOTrainer**     | Implementa el algoritmo PPO con Clipped Surrogate Objective y Generalized Advantage Estimation (GAE). | `trainers/ppo_trainer.py`            |
| **Replay Buffer**  | Almacena transiciones `(estado, acci√≥n, recompensa, next_state, done)` para entrenamiento por lotes. | Dentro de `ppo_agent.py` (integrado) |
| **Entorno (Gym)**  | Proporciona las observaciones y recompensas en cada paso.   

### CNN Visual

Usamos una arquitectura CNN para extraer caracter√≠sticas visuales de stacks de frames RGB (input: (12, 84, 84)):

